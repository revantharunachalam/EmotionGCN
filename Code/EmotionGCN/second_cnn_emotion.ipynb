{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np                     \n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import random\n",
    "import torchvision\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import json\n",
    "from os import listdir\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('done loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D,MaxPool2D,Dropout,Activation,LeakyReLU\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array,ImageDataGenerator \n",
    "from tensorflow.keras.preprocessing import image  \n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "print('done loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folders_in_path(path): #takes path as input\n",
    "    if not Path.is_dir(path): #checks if path exsist\n",
    "        raise ValueError(\"argument is not directory\") #produses error\n",
    "        #if not in directory\n",
    "    yield from filter(Path.is_dir,path.iterdir())\n",
    "def folders_in_depth(path,depth):\n",
    "    if 0>depth:\n",
    "        raise ValueError(\"depth smaller 0\")\n",
    "    if 0==depth:\n",
    "        yield from folders_in_path(path)\n",
    "    else:\n",
    "        for folder in folders_in_path(path):\n",
    "            yield from folders_in_depth(folder,depth-1)\n",
    "def files_in_path(path):\n",
    "    if not Path.is_dir(path):\n",
    "        raise ValueError(\"argument is not a directory\")\n",
    "    yield from filter(Path.is_file,path.iterdir())\n",
    "def sum_file_size(filepaths):\n",
    "    return sum([filep.stat().st_size for filep in filepaths])\n",
    "def convert_image_to_array(image_dir):\n",
    "    try:\n",
    "        image = cv.imread(image_dir)\n",
    "        if image is not None :\n",
    "            image = cv.resize(image, default_image_size)   \n",
    "            return img_to_array(image)\n",
    "        else :\n",
    "            return np.array([])\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return None \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(path):\n",
    "    images = []\n",
    "    for label in labels:\n",
    "        direc = os.path.join(path, label)\n",
    "        class_num = labels.index(label)\n",
    "        for image in os.listdir(direc):\n",
    "            image_read = cv.imread(os.path.join(direc,image),cv.IMREAD_GRAYSCALE)\n",
    "            image_resized = cv.resize(image_read,(image_size,image_size))\n",
    "            images.append([image_resized,class_num])\n",
    "            \n",
    "    return np.array(images)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry', 'happy', 'sad', 'surprise']\n",
      "The labels are ['angry', 'happy', 'sad', 'surprise']\n",
      "Done loading all images\n",
      "Length of X = 19300\n",
      "Length of y = 19300\n",
      "Number of class 1 images = 3993\n",
      "Number of class 2 images = 7164\n",
      "Number of class 3 images = 4938\n"
     ]
    }
   ],
   "source": [
    "# looking into complete folder to understand number of images \n",
    "path = 'C:\\\\Users\\\\ACER\\\\Desktop\\\\Emotion_detection\\\\train_trim\\\\'\n",
    "train = os.listdir(path)\n",
    "folders=[]\n",
    "folders = [f for f in sorted(os.listdir(path))]\n",
    "print(folders)\n",
    "labels = folders\n",
    "print (f'The labels are {labels}')\n",
    "image_size = 56\n",
    "train_images = load_train(path)\n",
    "X = []\n",
    "y = []\n",
    "for feature, label in train_images:\n",
    "    X.append(feature)\n",
    "    y.append(label)\n",
    "print('Done loading all images')    \n",
    "print (f'Length of X = {len(X)}')\n",
    "print (f'Length of y = {len(y)}')\n",
    "## checking the number of images of each class\n",
    "a = 0\n",
    "b = 0\n",
    "c = 0\n",
    "d = 0\n",
    "e = 0\n",
    "for label in y:\n",
    "   if label == 0:\n",
    "        a += 1\n",
    "   if label == 1:\n",
    "       b += 1\n",
    "   if label == 2:\n",
    "       c += 1\n",
    "   if label == 3:\n",
    "       d += 1\n",
    "   if label == 4:\n",
    "       e += 1\n",
    "#        \n",
    "print (f'Number of class 1 images = {a}')\n",
    "print (f'Number of class 2 images = {b}')\n",
    "print (f'Number of class 3 images = {c}')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=path\n",
    "EPOCHS = 10 #50\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "default_image_size = tuple((156, 156))\n",
    "image_size = 0\n",
    "width=156\n",
    "height=156\n",
    "depth=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry', 'happy', 'sad', 'surprise']\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\n"
     ]
    }
   ],
   "source": [
    "image_list, label_list = [], []\n",
    "train_labels=os.listdir(train_path) #take training path labels\n",
    "train_labels.sort() #sort the labels\n",
    "print(train_labels) #primt the lables\n",
    "global_features=[] #initialize variable to combine all features\n",
    "labels=[] #create label variables so as to decode text to number\n",
    "total=0 #initialize\n",
    "tot_file=[] #initialize\n",
    "count=1 #start count to check number of images\n",
    "i=0\n",
    "j=0\n",
    "k=0\n",
    "print(Path.cwd()) #gives the current path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\.idea\\inspectionProfiles:filecount:1,total size:174\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\gcn1\\conda-meta:filecount:61,total size:11459445\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\gcn1\\DLLs:filecount:34,total size:10595549\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\gcn1\\include:filecount:102,total size:656144\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\gcn1\\Lib:filecount:172,total size:4325196\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\gcn1\\Library:filecount:0,total size:0\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\gcn1\\libs:filecount:3,total size:512288\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\gcn1\\Scripts:filecount:83,total size:13221247\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\gcn1\\share:filecount:0,total size:0\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\gcn1\\sip:filecount:0,total size:0\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\gcn1\\tcl:filecount:6,total size:1600642\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\gcn1\\Tools:filecount:0,total size:0\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train\\angry:filecount:3993,total size:6332643\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train\\happy:filecount:7164,total size:11296707\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train\\neutral:filecount:4982,total size:7740763\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train\\sad:filecount:4938,total size:7640144\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train\\surprise:filecount:3205,total size:5182401\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train_trim\\angry:filecount:3993,total size:6332643\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train_trim\\happy:filecount:7164,total size:11296707\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train_trim\\sad:filecount:4938,total size:7640144\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train_trim\\surprise:filecount:3205,total size:5182401\n"
     ]
    }
   ],
   "source": [
    "for folder in folders_in_depth(Path.cwd(),1):\n",
    "        #first loop will pick the first foldend then next folder\n",
    "        files=list(files_in_path(folder)) #list all files in folder\n",
    "        file=len(files) #length of files\n",
    "        tot_file.append(file) #because we are running for all folder\n",
    "        # we are appending all files in tot_file at the end we\n",
    "        #shall get the list of number of files in the folder\n",
    "        #we are doing this because every folder has different number of files\n",
    "        #at the end when we are trainig all class of disease have to be\n",
    "        #trained equally, hence find the least number of images in the folder\n",
    "        #and then train accordingly\n",
    "        total_size=sum_file_size(files)\n",
    "        #total size of files\n",
    "        count=count+1 #check total number of files executed\n",
    "        print(f'{folder}:filecount:{len(files)},total size:{total_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tOTAL FILE: [0, 0, 0, 0, 1, 3, 6, 34, 61, 83, 102, 172, 3205, 3205, 3993, 3993, 4938, 4938, 4982, 7164, 7164]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "tot_file.sort() #sort files based on ascending order\n",
    "num = tot_file[1]#tot_file[2] #Index 0 is junkhence extract index 1\n",
    "print('tOTAL FILE:',tot_file)\n",
    "print(num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_per_class=3200 #consider number of images per class\n",
    "#%%START WITH TRAINING\n",
    "#for tr_name in range(0,2):\n",
    "count=0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200\n"
     ]
    }
   ],
   "source": [
    "path1=path \n",
    "print(images_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train_trim\\angry\n",
      "[STATUS] processed folder: angry\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train_trim\\happy\n",
      "[STATUS] processed folder: happy\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train_trim\\sad\n",
      "[STATUS] processed folder: sad\n",
      "C:\\Users\\ACER\\Desktop\\Emotion_detection\\train_trim\\surprise\n",
      "[STATUS] processed folder: surprise\n",
      "[STATUS] training labels(12800,)\n"
     ]
    }
   ],
   "source": [
    "for count in range(0,len(train_labels)):\n",
    "   \n",
    "    tr_name=count\n",
    "    \n",
    "    dir=train_path+train_labels[tr_name]\n",
    "    print(dir)\n",
    "    current_label=train_labels[tr_name]\n",
    "    print(\"[STATUS] processed folder: {}\".format(current_label))\n",
    "    k=1\n",
    "    file_sub_folder=os.listdir(dir) \n",
    "    for x in range(0,images_per_class):\n",
    "        file=dir +'/'+ file_sub_folder[x]\n",
    "#         print(file)    \n",
    "        image_list.append(convert_image_to_array(file ))\n",
    "        label_list.append(current_label) \n",
    "        i+=1\n",
    "        k+=1    \n",
    "        count=count+1\n",
    "print(\"[STATUS] training labels{}\".format(np.array(label_list).shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800\n",
      "['angry' 'happy' 'sad' 'surprise']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "image_size = len(image_list)\n",
    "print(image_size)\n",
    "label_binarizer = LabelBinarizer()\n",
    "image_labels = label_binarizer.fit_transform(label_list)\n",
    "pickle.dump(label_binarizer,open('label_transform.pkl', 'wb'))\n",
    "n_classes = len(label_binarizer.classes_)\n",
    "print(label_binarizer.classes_)\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800\n"
     ]
    }
   ],
   "source": [
    "print(len(image_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Spliting data to train, test\n"
     ]
    }
   ],
   "source": [
    "np_image_list = np.array(image_list, dtype=np.float16) / 225.0\n",
    "print(\"[INFO] Spliting data to train, test\")\n",
    "x_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.2, random_state = 42) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt ='.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 155, 155, 2)       26        \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 155, 155, 2)       8         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 155, 155, 2)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 38, 38, 2)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 35, 35, 4)         132       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 35, 35, 4)         16        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 35, 35, 4)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 17, 17, 4)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1156)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 4628      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 4,810\n",
      "Trainable params: 4,798\n",
      "Non-trainable params: 12\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model\n",
    "input_shape=x_test[0].shape\n",
    "model = Sequential()\n",
    "model.add(Conv2D(2, kernel_size=(2, 2), activation='relu', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPool2D(pool_size=(4,4)))\n",
    "model.add(Conv2D(4, kernel_size=(4, 4), activation='relu', input_shape=input_shape))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.2))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "# Compile the model\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 1],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10240 samples\n",
      "Epoch 1/10\n",
      "10240/10240 [==============================] - 109s 11ms/sample - loss: 0.5882 - acc: 0.7443\n",
      "Epoch 2/10\n",
      "10240/10240 [==============================] - 107s 10ms/sample - loss: 0.5619 - acc: 0.7502\n",
      "Epoch 3/10\n",
      "10240/10240 [==============================] - 105s 10ms/sample - loss: 0.5613 - acc: 0.7503\n",
      "Epoch 4/10\n",
      "10240/10240 [==============================] - 106s 10ms/sample - loss: 0.5604 - acc: 0.7506\n",
      "Epoch 5/10\n",
      "10240/10240 [==============================] - 106s 10ms/sample - loss: 0.5593 - acc: 0.7511\n",
      "Epoch 6/10\n",
      "10240/10240 [==============================] - 105s 10ms/sample - loss: 0.5579 - acc: 0.7512\n",
      "Epoch 7/10\n",
      "10240/10240 [==============================] - 105s 10ms/sample - loss: 0.5555 - acc: 0.7522\n",
      "Epoch 8/10\n",
      "10240/10240 [==============================] - 105s 10ms/sample - loss: 0.5524 - acc: 0.7531\n",
      "Epoch 9/10\n",
      "10240/10240 [==============================] - 105s 10ms/sample - loss: 0.5484 - acc: 0.7554\n",
      "Epoch 10/10\n",
      "10240/10240 [==============================] - 105s 10ms/sample - loss: 0.5443 - acc: 0.7566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f002cadf60>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       2\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "2555    0\n",
       "2556    0\n",
       "2557    0\n",
       "2558    0\n",
       "2559    0\n",
       "Length: 2560, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "pred = np.round(model.predict(x_test))\n",
    "test_labels = pd.DataFrame(y_test).idxmax(axis=1)\n",
    "predictions = pd.DataFrame(pred).idxmax(axis=1)\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix= confusion_matrix(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(confusion_matrix,['angry' 'happy' 'sad' 'surprise'],normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 25.9765625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(test_labels, predictions)\n",
    "print('Accuracy:',accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    " \n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry' 'happy' 'sad' 'surprise']\n",
      "[INFO] Spliting data to train, test\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 156, 156, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 156, 156, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 156, 156, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 52, 52, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 52, 52, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 52, 52, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 52, 52, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 52, 52, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 52, 52, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 26, 26, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 26, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 26, 26, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 26, 26, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 21632)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              22152192  \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 4100      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 22,439,812\n",
      "Trainable params: 22,436,932\n",
      "Non-trainable params: 2,880\n",
      "_________________________________________________________________\n",
      "[INFO] training network...\n",
      "Epoch 1/10\n",
      "320/320 [==============================] - 1259s 4s/step - loss: 0.6927 - acc: 0.7021 - val_loss: 0.7498 - val_acc: 0.6916\n",
      "Epoch 2/10\n",
      "320/320 [==============================] - 1206s 4s/step - loss: 0.6128 - acc: 0.7213 - val_loss: 0.6139 - val_acc: 0.7401\n",
      "Epoch 3/10\n",
      "320/320 [==============================] - 5057s 16s/step - loss: 0.5704 - acc: 0.7372 - val_loss: 0.4926 - val_acc: 0.7746\n",
      "Epoch 4/10\n",
      "320/320 [==============================] - 1197s 4s/step - loss: 0.5332 - acc: 0.7544 - val_loss: 0.5803 - val_acc: 0.7333\n",
      "Epoch 5/10\n",
      "320/320 [==============================] - 1741s 5s/step - loss: 0.5255 - acc: 0.7572 - val_loss: 0.5270 - val_acc: 0.7635\n",
      "Epoch 6/10\n",
      "320/320 [==============================] - 1205s 4s/step - loss: 0.5035 - acc: 0.7671 - val_loss: 0.4752 - val_acc: 0.7753\n",
      "Epoch 7/10\n",
      "320/320 [==============================] - 1191s 4s/step - loss: 0.4807 - acc: 0.7775 - val_loss: 0.6864 - val_acc: 0.7326\n",
      "Epoch 8/10\n",
      "320/320 [==============================] - 1189s 4s/step - loss: 0.4668 - acc: 0.7859 - val_loss: 0.4363 - val_acc: 0.7956\n",
      "Epoch 9/10\n",
      "320/320 [==============================] - 1193s 4s/step - loss: 0.4514 - acc: 0.7911 - val_loss: 0.4929 - val_acc: 0.7771\n",
      "Epoch 10/10\n",
      "320/320 [==============================] - 1197s 4s/step - loss: 0.4464 - acc: 0.7933 - val_loss: 0.5791 - val_acc: 0.7310\n"
     ]
    }
   ],
   "source": [
    "image_size = len(image_list)\n",
    "label_binarizer = LabelBinarizer()\n",
    "image_labels = label_binarizer.fit_transform(label_list)\n",
    "pickle.dump(label_binarizer,open('label_transform.pkl', 'wb'))\n",
    "n_classes = len(label_binarizer.classes_)\n",
    "print(label_binarizer.classes_)\n",
    "np_image_list = np.array(image_list, dtype=np.float16) / 225.0\n",
    "print(\"[INFO] Spliting data to train, test\")\n",
    "x_train, x_test, y_train, y_test = train_test_split(np_image_list, image_labels, test_size=0.2, random_state = 42) \n",
    "aug = ImageDataGenerator(\n",
    "rotation_range=25, width_shift_range=0.1,\n",
    "height_shift_range=0.1, shear_range=0.2, \n",
    "zoom_range=0.2,horizontal_flip=True, \n",
    "fill_mode=\"nearest\")\n",
    "model = Sequential()\n",
    "inputShape = (height, width, 3)\n",
    " \n",
    "chanDim = -1\n",
    "if K.image_data_format() == \"channels_first\":\n",
    "    inputShape = (depth, height, width)\n",
    "    chanDim = 1\n",
    "model.add(Conv2D(32,(3, 3), padding=\"same\",input_shape=inputShape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_classes))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.summary()\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "# distribution\n",
    " \n",
    "model.compile(optimizer= \"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "\n",
    "history = model.fit_generator(\n",
    "    aug.flow(x_train, y_train, batch_size=BS),\n",
    "    validation_data=(x_test, y_test),\n",
    "    steps_per_epoch=len(x_train) // BS,\n",
    "    epochs=EPOCHS, verbose=1\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Calculating model accuracy\n",
      "2560/2560 [==============================] - 124s 48ms/step\n",
      "Test Accuracy: 73.095703125\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] Calculating model accuracy\")\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {scores[1]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
